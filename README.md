# **Agentic Data Pipeline: An Architectural Overview**

<p align="center">
  <img src="demo/workflowDiagram.png" alt="Agentic Data Pipeline Architecture" width="800"/>
</p>

<p align="center">
    <a href="https://github.com/RauhanAhmed/AgenticDataPipeline">
      <img alt="GitHub Repo" src="https://img.shields.io/badge/GitHub-Repository-blue?style=for-the-badge&logo=github">
    </a>
    <img alt="Python Version" src="https://img.shields.io/badge/Python-3.12+-blue?style=for-the-badge&logo=python">
    <img alt="License" src="https://img.shields.io/badge/License-MIT-yellow?style=for-the-badge">
</p>

<p align="center">
    <a href="https://agenticdatapipeline.lovable.app/">
      <img alt="Frontend" src="https://img.shields.io/badge/Live-Frontend-brightgreen?style=for-the-badge">
    </a>
    <a href="https://rauhan-agenticdatapipeline.hf.space/docs">
      <img alt="Backend" src="https://img.shields.io/badge/Live-Backend-orange?style=for-the-badge">
    </a>
</p>

---

# **ğŸ“‘ Table of Contents**

1. [Introduction & Project Philosophy](#1-introduction--project-philosophy)
2. [Directory Structure](#2-directory-structure)
3. [System Architecture & Data Flow](#3-system-architecture--data-flow)
4. [Agent Details](#4-agent-details)
5. [Technology Stack](#5-technology-stack)
6. [Visual Documentation](#6-visual-documentation)
7. [Local Installation Guide](#7-local-installation-guide)
8. [Roadmap](#8-roadmap)
9. [Author](#9-author)
10. [License](#10-license)

---

# **1. Introduction & Project Philosophy**

Working with data usually means dealing with scattered sources: PDFs, wiki pages, SQL databases, CSVs, and the internet. Answering a real question often means pulling something from each of them, understanding the context, and then combining it all into a clear answer.

The **Agentic Data Pipeline** is built around this idea. Instead of relying on a single â€œsuper model,â€ the system uses **multiple small, focused AI agents**, each good at one thingâ€”document retrieval, SQL queries, web search, or reasoning. Their outputs are combined by a final synthesizer agent that produces a single coherent answer.

The goal is simple:

**Break the problem into specialized steps, run them in parallel, and combine the results intelligently.**

## **1.1. Live Demonstrations**

* **Frontend UI:** [https://agenticdatapipeline.lovable.app](https://agenticdatapipeline.lovable.app)
* **Backend API (Swagger):** [https://rauhan-agenticdatapipeline.hf.space/docs](https://rauhan-agenticdatapipeline.hf.space/docs)

---

# **2. Directory Structure**

Only core project files are included â€” unnecessary system files (`.DS_Store`, `.venv/`, caches) have been removed.

```
AgenticDataPipeline/
â”œâ”€â”€ LICENSE
â”œâ”€â”€ README.md
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ prompts.yaml
â”œâ”€â”€ config.ini
â”œâ”€â”€ .gitignore
â”œâ”€â”€ .python-version
â”œâ”€â”€ main.py
â”‚
â”œâ”€â”€ demo/
â”‚   â”œâ”€â”€ demo1.png
â”‚   â”œâ”€â”€ demo2.png
â”‚   â”œâ”€â”€ demo3.png
â”‚   â”œâ”€â”€ demo4.png
â”‚   â”œâ”€â”€ demo5.png
â”‚   â”œâ”€â”€ workflowDiagram.png
â”‚   â”œâ”€â”€ frontend.png
â”‚   â”œâ”€â”€ fastapiSwaggerUI.png
â”‚   â”œâ”€â”€ langsmithDashboard.png
â”‚   â”œâ”€â”€ langsmithTracing.png
â”‚   â”œâ”€â”€ langgrapphMermaidExport.png
â”‚   â”œâ”€â”€ postgreSQLAgentData.png
â”‚   â””â”€â”€ qdrantVectoDB.png
â”‚
â”œâ”€â”€ internalDocs/
â”‚   â”œâ”€â”€ ai_job_dataset.csv
â”‚   â”œâ”€â”€ Algorithm - Wikipedia.pdf
â”‚   â”œâ”€â”€ Computer science - Wikipedia.pdf
â”‚   â”œâ”€â”€ Data structure - Wikipedia.pdf
â”‚   â”œâ”€â”€ Database - Wikipedia.pdf
â”‚   â”œâ”€â”€ Operating system - Wikipedia.pdf
â”‚   â””â”€â”€ Programming language - Wikipedia.pdf
â”‚
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ logger.py
â”‚   â”œâ”€â”€ exceptions.py
â”‚   â””â”€â”€ initMethods.py
â”‚
â”œâ”€â”€ api/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ models.py
â”‚   â””â”€â”€ services.py
â”‚
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ VectorDBPopulator.ipynb
â”‚   â”œâ”€â”€ SQLPoplulator.ipynb
â”‚   â””â”€â”€ IndividualAgents.ipynb
â”‚
â””â”€â”€ src/
    â”œâ”€â”€ __init__.py
    â”œâ”€â”€ workflows/
    â”‚   â”œâ”€â”€ __init__.py
    â”‚   â””â”€â”€ workflow.py
    â””â”€â”€ components/
        â”œâ”€â”€ __init__.py
        â”œâ”€â”€ ragAgent.py
        â”œâ”€â”€ sqlAgent.py
        â”œâ”€â”€ reasoningAgent.py
        â”œâ”€â”€ internetSearchAgent.py
        â””â”€â”€ synthesizerAgent.py
```

---

# **3. System Architecture & Data Flow**

The system is implemented as a **stateful Directed Acyclic Graph (DAG)** using **LangGraph**, with **FastAPI** as the public-facing interface.

## **3.1. AgentState**

All agents exchange information through a shared state object:

```python
class AgentState(TypedDict):
    query: str
    ragResults: Optional[str]
    sqlResults: Optional[str]
    webResults: Optional[str]
    reasoningResults: Optional[str]
    finalAnswer: Optional[str]
```

## **3.2. End-to-End Flow**

1. The `/answerQuery` API receives a query.
2. A fresh AgentState is created.
3. LangGraph fans out into **four parallel agents**:

   * RAG
   * PostgreSQL
   * Internet Search
   * Reasoning
4. Each agent writes back into the state.
5. LangGraph waits until all four complete.
6. The **Synthesizer Agent** merges all outputs.
7. The final answer is returned.

---

# **4. Agent Details**

## **4.1. RAG Agent (â€œThe Librarianâ€)**

* **Model:** Llama 3.3 70B
* Works with internal PDFs, wikis, and other documents
* Retrieval:

  * Dense: `BAAI/bge-large-en-v1.5` â†’ Qdrant
  * Sparse: BM25
* Output â†’ `ragResults`

## **4.2. PostgreSQL Agent (â€œThe Data Whizâ€)**

* **Model:** zai-glm-4.6
* Natural-language-to-SQL
* Uses SQLAlchemy + LangChain SQL toolkit
* Output â†’ `sqlResults`

## **4.3. Reasoning Agent (â€œThe Thinkerâ€)**

* **Model:** qwen-3-32b
* Uses chain-of-thought reasoning
* Output â†’ `reasoningResults`

## **4.4. Internet Search Agent (â€œThe Web Surferâ€)**

* Uses Google Serper API
* Output â†’ `webResults`

## **4.5. Synthesizer Agent (â€œThe Editor-in-Chiefâ€)**

* **Model:** gpt-oss-120b
* Merges everything using:

  1. Internal docs + SQL
  2. Internet Search
  3. Reasoning
* Output â†’ `finalAnswer`

---

# **5. Technology Stack**

| Layer       | Technology       | Reason                          |
| ----------- | ---------------- | ------------------------------- |
| API         | FastAPI, Uvicorn | Fast, async, clean docs         |
| Workflow    | LangGraph        | Parallel stateful orchestration |
| VectorDB    | Qdrant           | Dense + sparse hybrid retrieval |
| DB          | PostgreSQL       | Reliable relational backend     |
| LLM Hosting | Cerebras         | Fast and affordable inference   |
| Containers  | Docker           | Reproducible deployments        |

---

# **6. Visual Documentation**

## **6.1. Sample Outputs**

<p align="center">
  <img src="demo/demo1.png" width="800"/><br>
  <img src="demo/demo2.png" width="800"/><br>
  <img src="demo/demo3.png" width="800"/><br>
  <img src="demo/demo4.png" width="800"/><br>
  <img src="demo/demo5.png" width="800"/>
</p>

## **6.2. Frontend & API**

<p align="center">
  <b>Frontend Interface</b><br>
  <img src="demo/frontend.png" width="800"/>
</p>

<p align="center">
  <b>FastAPI Swagger UI</b><br>
  <img src="demo/fastapiSwaggerUI.png" width="800"/>
</p>

## **6.3. Workflow & Architecture**

<p align="center">
  <img src="demo/workflowDiagram.png" width="800"/><br>
  <img src="demo/langgrapphMermaidExport.png" width="800"/>
</p>

## **6.4. Storage & Retrieval Layers**

<p align="center">
  <img src="demo/qdrantVectoDB.png" width="800"/>
</p>

<p align="center">
  <img src="demo/postgreSQLAgentData.png" width="800"/>
</p>

## **6.5. Monitoring & Tracing**

<p align="center">
  <img src="demo/langsmithDashboard.png" width="800"/><br>
  <img src="demo/langsmithTracing.png" width="800"/>
</p>

---

# **7. Local Installation Guide**

## **7.1. Requirements**

* Python 3.12+
* Docker Engine

## **7.2. Installation Steps**

### **1. Clone the repo**

```bash
git clone https://github.com/RauhanAhmed/AgenticDataPipeline.git
cd AgenticDataPipeline
```

### **2. Create `.env`**

Youâ€™ll need keys for:

* QDRANT_API_KEY
* QDRANT_URL
* POSTGRE_CONNECTION_STRING
* CEREBRAS_API_KEY
* SERPER_API_KEY
* Additional LLM keys as needed

### **3. Create virtual environment**

```bash
python3 -m venv .venv
source .venv/bin/activate
pip install -r requirements.txt
```

### **4. Build & run Docker**

```bash
docker build -t agentic-data-pipeline .
docker run -p 7860:7860 -d --env-file .env agentic-data-pipeline
```

## **7.3. Populate Data**

### **Vector Database (Qdrant)**

```
notebooks/VectorDBPopulator.ipynb
```

### **PostgreSQL Database**

```
notebooks/SQLPoplulator.ipynb
```

## **7.4. Test the Pipeline**

```bash
curl -X POST http://localhost:7860/answerQuery \
  -H "Content-Type: application/json" \
  -d '{"query": "How many data scientist jobs are available in California?"}'
```

---

# **8. Roadmap**

* [ ] Add Reciprocal Rank Fusion (RRF)
* [ ] Feedback-driven fine-tuning loop
* [ ] New specialized agents (filesystem, JIRA, etc.)
* [ ] Optional Streamlit/Gradio UI

---

# **9. Author**

Built by **Rauhan Ahmed**
Portfolio: [https://rauhanahmed.in](https://rauhanahmed.in)

Contributions welcome â€” feel free to open an issue or PR.

---

# **10. License**

MIT License. See `LICENSE`.

---